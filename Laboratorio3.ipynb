{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab III - Advanced Topics**\n",
    "\n",
    "Workshop III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *In your own words, describe what vector embeddings are and what they are useful for*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los embeddings son como mapas que convierten palabras, oraciones e incluso im치genes en puntos en un espacio 3D. Estos puntos se agrupan seg칰n su significado, permitiendonos usar la logica matem치ticas para compararlos y analizarlos. Son muy 칰tiles para b칰squedas en internet, traducci칩n autom치tica, generaci칩n de im치genes a partir de texto y control de robots. En resumen, los embeddings son una herramienta poderosa para que las m치quinas comprendan el mundo que nos rodea.\n",
    "\n",
    "Dicha de otra manera Imagina que tienes una biblioteca gigante con miles de libros. Los embeddings te ayudan a encontrar el libro que buscas sin tener que leer cada p치gina. Simplemente describes el libro que necesitas, y el embedding te gu칤a hacia la secci칩n de la biblioteca donde se encuentra. Pareciera que tiene un bibliotecario experto a su disposici칩n.\n",
    "\n",
    "Los embeddings est치n cambiando la forma en que las m치quinas interact칰an con el mundo. Son una herramienta poderosa que nos permite aprovechar el poder de la inteligencia artificial para resolver problemas complejos y mejorar nuestras vidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each  other? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de seleccionar el criterio 칩ptimo para medir la distancia entre nuestros vectores de incrustaci칩n, se plantea una dicotom칤a cl치sica entre la distancia euclidiana y la distancia del coseno. Esta elecci칩n se asemeja a la decisi칩n entre calcular la distancia en l칤nea recta, una opci칩n m치s directa y concisa (distancia euclidiana), o considerar el 치ngulo entre los vectores, valorando la similitud en la direcci칩n de sus movimientos (distancia del coseno). Podr칤amos ilustrar esta disyuntiva imaginando la eficiencia de un trayecto directo entre estantes de libros en una biblioteca frente a una ruta m치s sutil y circular. En el 치mbito de las incrustaciones de texto, donde el inter칠s radica en capturar la esencia de la similitud sem치ntica, la preferencia por la distancia del coseno es evidente. Esta m칠trica atiende al estilo del baile m치s que a los movimientos individuales, convirti칠ndola en una opci칩n clave cuando la atenci칩n recae en la esencia de la similitud, especialmente en el din치mico y dimensionalmente elevado terreno del procesamiento del lenguaje natural. As칤, la elecci칩n entre una l칤nea recta o un arco elegante depende del ritmo inherente a los datos y de la coreograf칤a espec칤fica de la tarea en cuesti칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let us build a Q&A (question answering) system! 游For this, consider the following steps:\n",
    "a. Pick whatever text you like, in the order of 20+ paragraphs\n",
    "b. Split that text into meaningful chunks/pieces\n",
    "c. Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?\n",
    "d. For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?\n",
    "4. What do you think that could make these types of systems more robust in terms of semantics and functionality?\n",
    "5. Bonus points if deployed on a local or cloud server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch #Libreria para operaciones de aprendizaje profundo\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering #tokenizador asociado al modelo y modelo pre-entrenado para tareas de respuesta a preguntas\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F #c치lculo de la similitud del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel:\n",
    "    def __init__(self, model_name):\n",
    "        # Inicializa el modelo de pregunta y respuesta\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)# este es el tokenizador y carga el modelo \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # determina si se utilizar치 la GPU o la CPU. El modelo se mueve a la GPU si est치 disponible.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    # Toma un texto como entrada, tokeniza el texto utilizando el tokenizador, y obtiene los embeddings utilizando el modelo de pregunta y respuesta\n",
    "    def generate_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)#Mueve la entrada tokenizada al dispositivo elegido\n",
    "        inputs.to(self.device)\n",
    "        with torch.no_grad(): # Desactiva el c치lculo del gradiente para mayor eficiencia\n",
    "            outputs = self.model(**inputs) #outputs = self.model(inputs)\n",
    "            # Corregir a 'pooler_output' en lugar de 'last_hidden_state'\n",
    "            # Calcula el embeding del p치rrafo utilizando el agrupamiento medio sobre el 칰ltimo estado oculto\n",
    "            embeddings = outputs.pooler_output.squeeze() # embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings.to(self.device)\n",
    "    # Utiliza la similitud del coseno para comparar los embeddings del input del usuario con los embeddings de cada p치rrafo.\n",
    "    def find_best_match(self, user_input, paragraphs, k=3):\n",
    "        user_embedding = self.generate_embedding(user_input)#Genera un embedings para la entrada del usuario utilizando\n",
    "        best_matches = [] # Inicializa una lista vac칤a\n",
    "        for i, paragraph in enumerate(paragraphs): # Itera sobre cada p치rrafo\n",
    "            paragraph_embedding = self.generate_embedding(paragraph)\n",
    "            # Calcula la similitud del coseno entre las incrustaciones del usuario y del p치rrafo usando (F.cosine_similarity)\n",
    "            similarity = F.cosine_similarity(user_embedding.unsqueeze(0), paragraph_embedding.unsqueeze(0), dim=1).item()\n",
    "            best_matches.append((i, similarity))#A침ade el 칤ndice del p치rrafo y la similitud\n",
    "        #best_matches.sort(key=lambda x: x[1], reverse=True) #Ordena la lista\n",
    "        best_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return best_matches[:k] # Devuelve las k mejores coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Cargar el texto desde un archivo\n",
    "    file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Parrafo.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texto = file.read() # Se almacena en la variable texto\n",
    "    all_answers = []\n",
    "    # Dividir el texto en p치rrafos significativos\n",
    "    paragraphs = [p.strip() for p in texto.split(\"\\n\") if p.strip()] # Se divide en p치rrafos y se almacena en la lista\n",
    "\n",
    "    qa_model = QAModel(\"deepset/roberta-base-squad2\") # Crear instancia del modelo\n",
    "    #carga, lee y almacena las preguntas en questions\n",
    "    questions_file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Preguntas.json'\n",
    "    with open(questions_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        questions = data[\"preguntas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        # Para cada pregunta, se llama a la funci칩n find_best_match del modelo para encontrar las k mejores coincidencias con los p치rrafos en la lista paragraphs\n",
    "        best_matches = qa_model.find_best_match(question, paragraphs)\n",
    "        best_paragraph_index = best_matches[0][0]\n",
    "        best_answer = paragraphs[best_paragraph_index]\n",
    "        all_answers.append({\"Pregunta\": question, \"Respuesta\": best_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacena cada Respuesta en un archivo json\n",
    "respuesta_file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Respuesta.json'\n",
    "with open(respuesta_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_answers, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LabUlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
