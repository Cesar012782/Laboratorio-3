{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab III - Advanced Topics**\n",
    "\n",
    "Workshop III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *In your own words, describe what vector embeddings are and what they are useful for*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los embeddings son como mapas que convierten palabras, oraciones e incluso imágenes en puntos en un espacio 3D. Estos puntos se agrupan según su significado, permitiendonos usar la logica matemáticas para compararlos y analizarlos. Son muy útiles para búsquedas en internet, traducción automática, generación de imágenes a partir de texto y control de robots. En resumen, los embeddings son una herramienta poderosa para que las máquinas comprendan el mundo que nos rodea.\n",
    "\n",
    "Dicha de otra manera Imagina que tienes una biblioteca gigante con miles de libros. Los embeddings te ayudan a encontrar el libro que buscas sin tener que leer cada página. Simplemente describes el libro que necesitas, y el embedding te guía hacia la sección de la biblioteca donde se encuentra. Pareciera que tiene un bibliotecario experto a su disposición.\n",
    "\n",
    "Los embeddings están cambiando la forma en que las máquinas interactúan con el mundo. Son una herramienta poderosa que nos permite aprovechar el poder de la inteligencia artificial para resolver problemas complejos y mejorar nuestras vidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each  other? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de seleccionar el criterio óptimo para medir la distancia entre nuestros vectores de incrustación, se plantea una dicotomía clásica entre la distancia euclidiana y la distancia del coseno. Esta elección se asemeja a la decisión entre calcular la distancia en línea recta, una opción más directa y concisa (distancia euclidiana), o considerar el ángulo entre los vectores, valorando la similitud en la dirección de sus movimientos (distancia del coseno). Podríamos ilustrar esta disyuntiva imaginando la eficiencia de un trayecto directo entre estantes de libros en una biblioteca frente a una ruta más sutil y circular. En el ámbito de las incrustaciones de texto, donde el interés radica en capturar la esencia de la similitud semántica, la preferencia por la distancia del coseno es evidente. Esta métrica atiende al estilo del baile más que a los movimientos individuales, convirtiéndola en una opción clave cuando la atención recae en la esencia de la similitud, especialmente en el dinámico y dimensionalmente elevado terreno del procesamiento del lenguaje natural. Así, la elección entre una línea recta o un arco elegante depende del ritmo inherente a los datos y de la coreografía específica de la tarea en cuestión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let us build a Q&A (question answering) system! 😀For this, consider the following steps:\n",
    "a. Pick whatever text you like, in the order of 20+ paragraphs\n",
    "b. Split that text into meaningful chunks/pieces\n",
    "c. Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?\n",
    "d. For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?\n",
    "4. What do you think that could make these types of systems more robust in terms of semantics and functionality?\n",
    "5. Bonus points if deployed on a local or cloud server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch #Libreria para operaciones de aprendizaje profundo\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering #tokenizador asociado al modelo y modelo pre-entrenado para tareas de respuesta a preguntas\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F #cálculo de la similitud del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel:\n",
    "    def __init__(self, model_name):\n",
    "        # Inicializa el modelo de pregunta y respuesta\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)# este es el tokenizador y carga el modelo \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # determina si se utilizará la GPU o la CPU. El modelo se mueve a la GPU si está disponible.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    # Toma un texto como entrada, tokeniza el texto utilizando el tokenizador, y obtiene los embeddings utilizando el modelo de pregunta y respuesta\n",
    "    def generate_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)#Mueve la entrada tokenizada al dispositivo elegido\n",
    "        inputs.to(self.device)\n",
    "        with torch.no_grad(): # Desactiva el cálculo del gradiente para mayor eficiencia\n",
    "            outputs = self.model(**inputs) #outputs = self.model(inputs)\n",
    "            # Corregir a 'pooler_output' en lugar de 'last_hidden_state'\n",
    "            # Calcula el embeding del párrafo utilizando el agrupamiento medio sobre el último estado oculto\n",
    "            embeddings = outputs.pooler_output.squeeze() # embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings.to(self.device)\n",
    "    # Utiliza la similitud del coseno para comparar los embeddings del input del usuario con los embeddings de cada párrafo.\n",
    "    def find_best_match(self, user_input, paragraphs, k=3):\n",
    "        user_embedding = self.generate_embedding(user_input)#Genera un embedings para la entrada del usuario utilizando\n",
    "        best_matches = [] # Inicializa una lista vacía\n",
    "        for i, paragraph in enumerate(paragraphs): # Itera sobre cada párrafo\n",
    "            paragraph_embedding = self.generate_embedding(paragraph)\n",
    "            # Calcula la similitud del coseno entre las incrustaciones del usuario y del párrafo usando (F.cosine_similarity)\n",
    "            similarity = F.cosine_similarity(user_embedding.unsqueeze(0), paragraph_embedding.unsqueeze(0), dim=1).item()\n",
    "            best_matches.append((i, similarity))#Añade el índice del párrafo y la similitud\n",
    "        #best_matches.sort(key=lambda x: x[1], reverse=True) #Ordena la lista\n",
    "        best_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return best_matches[:k] # Devuelve las k mejores coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Cargar el texto desde un archivo\n",
    "    file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Parrafo.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texto = file.read() # Se almacena en la variable texto\n",
    "    all_answers = []\n",
    "    # Dividir el texto en párrafos significativos\n",
    "    paragraphs = [p.strip() for p in texto.split(\"\\n\") if p.strip()] # Se divide en párrafos y se almacena en la lista\n",
    "\n",
    "    qa_model = QAModel(\"deepset/roberta-base-squad2\") # Crear instancia del modelo\n",
    "    #carga, lee y almacena las preguntas en questions\n",
    "    questions_file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Preguntas.json'\n",
    "    with open(questions_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        questions = data[\"preguntas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        # Para cada pregunta, se llama a la función find_best_match del modelo para encontrar las k mejores coincidencias con los párrafos en la lista paragraphs\n",
    "        best_matches = qa_model.find_best_match(question, paragraphs)\n",
    "        best_paragraph_index = best_matches[0][0]\n",
    "        best_answer = paragraphs[best_paragraph_index]\n",
    "        all_answers.append({\"Pregunta\": question, \"Respuesta\": best_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacena cada Respuesta en un archivo json\n",
    "respuesta_file_path = 'C:\\\\Users\\\\Cesars\\\\OneDrive\\\\Escritorio\\\\Lab3\\\\Respuesta.json'\n",
    "with open(respuesta_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_answers, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LabUlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
